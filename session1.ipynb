{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Text Preprocessing Tutorial Using NLTK for Arabic and English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective:\n",
    "#### Learn to preprocess text (Arabic and English) using NLTK. The steps include:\n",
    "##### Tokenization\n",
    "##### Stopword removal\n",
    "##### Noise removal\n",
    "##### Normalization\n",
    "##### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Install and Import NLTK\n",
    "#### If you haven't installed NLTK, do so first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\abdulrahman_1114\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\abdulrahman_1114\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\abdulrahman_1114\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\abdulrahman_1114\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\abdulrahman_1114\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\abdulrahman_1114\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "# Install NLTK if not already installed\n",
    "!pip install nltk\n",
    "\n",
    "# Import required NLTK libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Download NLTK Resources\n",
    "#### Download the necessary corpora and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\abdulrahman_1114\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\abdulrahman_1114\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Preprocessing Steps\n",
    "#### 1. Tokenization\n",
    "#### Tokenization splits the text into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text, language='english'):\n",
    "    return word_tokenize(text, language='arabic' if language == 'arabic' else 'english')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stopword Removal\n",
    "#### Stopwords are common words that don't contribute much meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(words, language='english'):\n",
    "    stop_words = set(stopwords.words(language))\n",
    "    return [word for word in words if word.lower() not in stop_words]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Noise Removal\n",
    "#### Remove punctuation, numbers, and unnecessary characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\d+', '', text)      # Remove numbers\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Normalization\n",
    "#### For Arabic, normalize letters and remove diacritics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    # Remove diacritics\n",
    "    text = re.sub(r'[\\u064B-\\u065F]', '', text)\n",
    "    # Normalize Arabic letters\n",
    "    text = re.sub(r'[إأآا]', 'ا', text)\n",
    "    text = re.sub(r'ى', 'ي', text)\n",
    "    text = re.sub(r'ة', 'ه', text)\n",
    "    text = re.sub(r'[ء]', '', text)\n",
    "    return text.lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Stemming\n",
    "#### Apply stemming to reduce words to their root forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(words, language='english'):\n",
    "    if language == 'arabic':\n",
    "        stemmer = ISRIStemmer()\n",
    "    else:\n",
    "        stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(word) for word in words]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Combine All Steps into a Single Function\n",
    "#### Integrate all the preprocessing steps into one reusable function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, language='english'):\n",
    "    # Normalize text\n",
    "    text = normalize_text(text)\n",
    "    \n",
    "    # Remove noise\n",
    "    text = remove_noise(text)\n",
    "    \n",
    "    # Tokenize text\n",
    "    tokens = tokenize_text(text, language)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = remove_stopwords(tokens, language)\n",
    "    \n",
    "    # Stem words\n",
    "    tokens = stem_words(tokens, language)\n",
    "    \n",
    "    # Combine tokens back into a string\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Apply Preprocessing to Example Texts\n",
    "#### Example Texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_text = \"Wow! Loved this place. Bit far but worth it.\"\n",
    "arabic_text = \"مرحباً بالعالم! هذه جملة اختبار لمعالجة النصوص. أرقام مثل ١٢٣٤٥ أو رموز مثل $٪& تعتبر ضوضاء.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed English Text: wow love place bit far worth\n"
     ]
    }
   ],
   "source": [
    "# Process English Text\n",
    "processed_english = preprocess_text(english_text, language='english')\n",
    "print(\"Processed English Text:\", processed_english)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\abdulrahman_1114\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing Required Libraries\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "import nltk\n",
    "\n",
    "# Download required resources\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Function to preprocess Arabic text\n",
    "def preprocess_text_ar(text):\n",
    "    \"\"\"\n",
    "    Preprocess Arabic text by normalizing, removing stopwords, stemming, and cleaning.\n",
    "    \n",
    "    Parameters:\n",
    "    text (str): Input Arabic text\n",
    "    \n",
    "    Returns:\n",
    "    str: Preprocessed Arabic text\n",
    "    \"\"\"\n",
    "    # 1. Normalize Arabic text (remove diacritics and unify letters)\n",
    "    text = re.sub(r'[\\u064B-\\u065F]', '', text)  # Remove diacritics\n",
    "    text = re.sub(r'[إأآا]', 'ا', text)          # Normalize alef\n",
    "    text = re.sub(r'ة', 'ه', text)              # Normalize ta marbouta\n",
    "    text = re.sub(r'ى', 'ي', text)              # Normalize ya\n",
    "    text = re.sub(r'ؤ', 'و', text)              # Normalize waw\n",
    "    text = re.sub(r'[ء]', '', text)             # Remove hamza\n",
    "    \n",
    "    # 2. Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 3. Remove punctuation and numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)         # Remove punctuation\n",
    "    text = re.sub(r'\\d+', '', text)             # Remove numbers\n",
    "    \n",
    "    # 4. Tokenize words\n",
    "    words = text.split()  # Basic tokenization for Arabic\n",
    "    \n",
    "    # 5. Remove stopwords\n",
    "    arabic_stopwords = set(stopwords.words('arabic')) if 'arabic' in stopwords.fileids() else set(['في', 'من', 'على', 'إلى', 'عن', 'و', 'يا', 'لكن', 'هذا', 'ما'])\n",
    "    filtered_words = [word for word in words if word not in arabic_stopwords]\n",
    "    \n",
    "    # 6. Apply stemming\n",
    "    stemmer = ISRIStemmer()\n",
    "    stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
    "    \n",
    "    # 7. Join words back into a string\n",
    "    processed_text = ' '.join(stemmed_words)\n",
    "    \n",
    "    return processed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: مرحباً بالعالم! هذه جملة اختبار لمعالجة النصوص. أرقام مثل ١٢٣٤٥ أو رموز مثل $٪& تعتبر ضوضاء.\n",
      "Processed Text: رحب علم جمل خبر علج نصص رقم او رمز عبر ضوض\n"
     ]
    }
   ],
   "source": [
    "# Example Arabic text\n",
    "arabic_text = \"مرحباً بالعالم! هذه جملة اختبار لمعالجة النصوص. أرقام مثل ١٢٣٤٥ أو رموز مثل $٪& تعتبر ضوضاء.\"\n",
    "\n",
    "# Preprocessing the Arabic text\n",
    "processed_text = preprocess_text_ar(arabic_text)\n",
    "\n",
    "# Display the result\n",
    "print(\"Original Text:\", arabic_text)\n",
    "print(\"Processed Text:\", processed_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Expected Outputs\n",
    "#### Original Texts:\n",
    "#### English: \"Wow! Loved this place. Bit far but worth it.\"\n",
    "#### Arabic: \"مرحباً بالعالم! هذه جملة اختبار لمعالجة النصوص. أرقام مثل ١٢٣٤٥ أو رموز مثل $٪& تعتبر ضوضاء.\"\n",
    "#### Processed Texts:\n",
    "#### English: wow love place bit far worth\n",
    "#### Arabic: رحب عالم جمل خبر علج نصوص"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of Steps\n",
    "#### Normalize text for consistency.\n",
    "#### Remove noise like punctuation and numbers.\n",
    "#### Tokenize text into words.\n",
    "#### Remove stopwords to focus on meaningful words.\n",
    "#### Apply stemming to reduce words to their root forms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### شرح مصطلحات **Text Preprocessing** بطريقة مبسطة (بالعربي والمصري):  \n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Tokenization (التقطيع النصي)**  \n",
    "**بالعربي**:  \n",
    "تقسيم النص لجمله أو كلمات صغيرة بحيث كل كلمة أو جملة تعتبر وحدة مستقلة.  \n",
    "زي لما تجيب كتاب وتقسمه لفصول أو صفحات عشان تفهمه أكتر.\n",
    "\n",
    "**بالمصري**:  \n",
    "بتقطع النص لجمل أو كلمات صغيرة عشان تبقى سهلة عليك تعالجها.  \n",
    "مثال:  \n",
    "جملة زي \"أنا بحب القهوة\" بتتقطع لـ [\"أنا\", \"بحب\", \"القهوة\"].\n",
    "\n",
    "**بالإنجليزي**:  \n",
    "Breaking a text into smaller units like sentences or words, making it easier to process.  \n",
    "Example:  \n",
    "A sentence like \"I love coffee\" becomes [\"I\", \"love\", \"coffee\"].  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Stopword Removal (إزالة الكلمات الشائعة)**  \n",
    "**بالعربي**:  \n",
    "إزالة الكلمات اللي بتتكرر كتير في النصوص بس مش بتضيف معنى قوي (زي: في، عن، إلى، the, is).  \n",
    "\n",
    "**بالمصري**:  \n",
    "دي الكلمات اللي ملهاش وزن تقيل في المعنى، بتشيلها عشان تركز على الكلام المهم.  \n",
    "زي: \"أنا رايح على الكافيه\" --> [\"رايح\", \"كافيه\"].\n",
    "\n",
    "**بالإنجليزي**:  \n",
    "Removing commonly used words that don't add much meaning to the text (e.g., \"in,\" \"on,\" \"the\").  \n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Noise Removal (إزالة التشويش)**  \n",
    "**بالعربي**:  \n",
    "تنضيف النصوص من الحاجات اللي مالهاش لازمة زي الأرقام، الرموز، أو العلامات الغريبة.  \n",
    "\n",
    "**بالمصري**:  \n",
    "زي لما تنظف أوضة وترمي الورق اللي مش محتاجه. هنا بنشيل الرموز زي \"#\"، والأرقام اللي ممكن متكنش مفيدة.  \n",
    "مثال: \"السلام عليكم 123 !!!\" --> \"السلام عليكم\".\n",
    "\n",
    "**بالإنجليزي**:  \n",
    "Cleaning the text by removing unnecessary elements like numbers, symbols, or punctuation marks.  \n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Normalization (التطبيع)**  \n",
    "**بالعربي**:  \n",
    "توحيد شكل النصوص عشان كله يبقى متطابق زي استخدام نفس الصيغة للأحرف، مثلاً (أ، إ، آ --> ا).  \n",
    "\n",
    "**بالمصري**:  \n",
    "تخلي النص كله شكله ثابت زي توحيد طريقة كتابة الكلمات. زي \"إلى\" و \"الي\" تبقى \"الى\".  \n",
    "\n",
    "**بالإنجليزي**:  \n",
    "Converting text into a consistent format by standardizing letters or removing accents.  \n",
    "Example:  \n",
    "Normalizing \"Éléphant\" to \"elephant.\"\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Stemming (التجذير)**  \n",
    "**بالعربي**:  \n",
    "إرجاع الكلمات لأصلها أو جذورها. زي تحويل \"تعلّموا\" إلى \"علم\".  \n",
    "\n",
    "**بالمصري**:  \n",
    "تجيب الكلمة من الآخر للجذر عشان تختصر المعنى. زي \"الكتابة\" تبقى \"كتب\".  \n",
    "\n",
    "**بالإنجليزي**:  \n",
    "Reducing words to their root or base form.  \n",
    "Example:  \n",
    "The words \"running,\" \"runner,\" \"ran\" are reduced to \"run.\"  \n",
    "\n",
    "---\n",
    "\n",
    "### **أمثلة توضيحية:**\n",
    "\n",
    "#### **Input Text:**\n",
    "- English:  \n",
    "  *\"I am going to the market. Shopping is fun!\"*  \n",
    "- Arabic:  \n",
    "  *\"أنا ذاهب إلى السوق. التسوق ممتع!\"*\n",
    "\n",
    "#### **Steps:**\n",
    "1. **Tokenization:**  \n",
    "   - English: [\"I\", \"am\", \"going\", \"to\", \"the\", \"market\", \".\", \"Shopping\", \"is\", \"fun\", \"!\"]  \n",
    "   - Arabic: [\"أنا\", \"ذاهب\", \"إلى\", \"السوق\", \".\", \"التسوق\", \"ممتع\", \"!\"]\n",
    "\n",
    "2. **Stopword Removal:**  \n",
    "   - English: [\"going\", \"market\", \"shopping\", \"fun\"]  \n",
    "   - Arabic: [\"ذاهب\", \"السوق\", \"التسوق\", \"ممتع\"]\n",
    "\n",
    "3. **Noise Removal:**  \n",
    "   - English: [\"going\", \"market\", \"shopping\", \"fun\"]  \n",
    "   - Arabic: [\"ذاهب\", \"السوق\", \"التسوق\", \"ممتع\"]\n",
    "\n",
    "4. **Normalization:**  \n",
    "   - Arabic: [\"ذاهب\", \"السوق\", \"تسوق\", \"ممتع\"]\n",
    "\n",
    "5. **Stemming:**  \n",
    "   - English: [\"go\", \"market\", \"shop\", \"fun\"]  \n",
    "   - Arabic: [\"ذهب\", \"سوق\", \"تسوق\", \"متع\"]\n",
    "\n",
    "#### **Final Preprocessed Text:**\n",
    "- English: \"go market shop fun\"  \n",
    "- Arabic: \"ذهب سوق تسوق متع\"  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
